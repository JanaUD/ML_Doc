{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJXDPbwkQ6Sz"
      },
      "outputs": [],
      "source": [
        "# 1. Instalación de dependencias\n",
        "!pip install polars scikit-learn matplotlib seaborn imbalanced-learn --quiet\n",
        "import os\n",
        "\n",
        "# 2. Importación de librerías\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from google.colab import files\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "\n",
        "# 3. Configuración de estilos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# 4. Función para cargar datos\n",
        "def load_data():\n",
        "    \"\"\"Carga datos con múltiples opciones.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" OPCIONES DE CARGA DE DATOS \".center(50, \"=\"))\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. Usar ruta por defecto (/content/Students_Grading_Dataset.csv)\")\n",
        "    print(\"2. Ingresar ruta manualmente\")\n",
        "    print(\"3. Subir archivo manualmente\")\n",
        "\n",
        "    choice = input(\"\\nSeleccione opción (1-3): \")\n",
        "\n",
        "    try:\n",
        "        if choice == \"1\":\n",
        "            path = \"/content/Students_Grading_Dataset.csv\"\n",
        "        elif choice == \"2\":\n",
        "            path = input(\"Ingrese la ruta completa al archivo CSV: \").strip()\n",
        "        elif choice == \"3\":\n",
        "            print(\"\\nPor favor, suba su archivo CSV:\")\n",
        "            uploaded = files.upload()\n",
        "            path = list(uploaded.keys())[0]\n",
        "        else:\n",
        "            raise ValueError(\"Opción no válida\")\n",
        "\n",
        "        df = pl.read_csv(path)\n",
        "        print(f\"\\n Datos cargados exitosamente desde: {path}\")\n",
        "        print(f\" Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error al cargar datos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# 5. Carga de datos\n",
        "try:\n",
        "    df = load_data()\n",
        "    print(\"\\n Muestra de datos (5 primeras filas):\")\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"\\n No se pudo cargar el archivo. Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# 6. Análisis Exploratorio Inicial\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" ANÁLISIS EXPLORATORIO \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 6.1 Tipos de datos\n",
        "plt.figure(figsize=(12, 4))\n",
        "dtypes = [str(dt) for dt in df.schema.values()]\n",
        "sns.barplot(x=list(df.columns), y=dtypes, palette=\"Blues_d\")\n",
        "plt.title('Tipos de Datos por Columna', pad=15)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Tipo de Dato')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6.2 Selección de variable objetivo\n",
        "def select_target(df):\n",
        "    \"\"\"Selección interactiva de variable objetivo.\"\"\"\n",
        "    numeric_cols = [col for col in df.columns\n",
        "                   if df[col].dtype in (pl.Float64, pl.Float32, pl.Int64, pl.Int32)]\n",
        "\n",
        "    if not numeric_cols:\n",
        "        raise ValueError(\"No se encontraron columnas numéricas\")\n",
        "\n",
        "    # Visualización de distribuciones\n",
        "    print(\"\\n Distribuciones de variables numéricas:\")\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, col in enumerate(numeric_cols, 1):\n",
        "        plt.subplot((len(numeric_cols)//3)+1, 3, i)\n",
        "        sns.histplot(df[col].to_numpy(), kde=True, color='skyblue')\n",
        "        plt.title(f'Distribución de {col}', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Selección interactiva\n",
        "    print(\"\\n Seleccione la columna objetivo:\")\n",
        "    for i, col in enumerate(numeric_cols, 1):\n",
        "        print(f\"{i}. {col}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            choice = int(input(\"\\nIngrese el número de la columna objetivo: \"))\n",
        "            if 1 <= choice <= len(numeric_cols):\n",
        "                selected_col = numeric_cols[choice-1]\n",
        "\n",
        "                if df[selected_col].dtype in (pl.Int32, pl.Int64):\n",
        "                    print(f\" Convirtiendo '{selected_col}' a float...\")\n",
        "                    df = df.with_columns(pl.col(selected_col).cast(pl.Float64))\n",
        "\n",
        "                return df, selected_col\n",
        "            print(\" Número fuera de rango. Intente nuevamente.\")\n",
        "        except ValueError:\n",
        "            print(\" Ingrese un número válido.\")\n",
        "\n",
        "try:\n",
        "    df, TARGET_COL = select_target(df)\n",
        "    print(f\"\\n Columna objetivo seleccionada: '{TARGET_COL}'\")\n",
        "\n",
        "    # Visualización de la variable objetivo\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(y=df[TARGET_COL].to_numpy(), color='lightblue')\n",
        "    plt.title('Diagrama de Caja')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(df[TARGET_COL].to_numpy(), kde=True, color='lightgreen')\n",
        "    plt.title('Distribución')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error en selección de variable objetivo: {e}\")\n",
        "    raise\n",
        "\n",
        "# 7. Ingeniería de Características\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" INGENIERÍA DE CARACTERÍSTICAS \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 7.1 Eliminar columnas irrelevantes\n",
        "irrelevant_cols = ['Student_ID', 'First_Name', 'Last_Name', 'Email']\n",
        "df = df.drop([col for col in irrelevant_cols if col in df.columns])\n",
        "\n",
        "# 7.2 Procesamiento de variables categóricas\n",
        "categorical_cols = [col for col in df.columns if df[col].dtype == pl.Utf8 and col != TARGET_COL]\n",
        "print(f\" Variables categóricas a transformar: {categorical_cols}\")\n",
        "\n",
        "if categorical_cols:\n",
        "    # Guardar nombres originales para referencia\n",
        "    original_categorical_cols = categorical_cols.copy()\n",
        "\n",
        "    # Aplicar one-hot encoding\n",
        "    df = df.to_dummies(columns=categorical_cols)\n",
        "\n",
        "    # Obtener nuevas columnas dummy creadas\n",
        "    dummy_cols = [col for col in df.columns\n",
        "                 if any(col.startswith(cat_col) for cat_col in original_categorical_cols)]\n",
        "\n",
        "    print(f\"\\n Nuevas columnas tras one-hot encoding: {df.shape[1]}\")\n",
        "    print(\"\\n Muestra de columnas transformadas:\")\n",
        "\n",
        "    # Mostrar solo algunas columnas dummy como ejemplo (máximo 5)\n",
        "    sample_dummy_cols = dummy_cols[:min(5, len(dummy_cols))]\n",
        "    print(df.select(sample_dummy_cols).head())\n",
        "\n",
        "# 8. Análisis Estadístico Completo\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" ANÁLISIS ESTADÍSTICO \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 8.1 Función para identificar columnas numéricas\n",
        "def get_numeric_cols(df):\n",
        "    \"\"\"Identifica columnas numéricas excluyendo la objetivo.\"\"\"\n",
        "    numeric_types = (pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
        "                    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
        "                    pl.Float32, pl.Float64)\n",
        "    return [col for col in df.columns\n",
        "            if df[col].dtype in numeric_types and col != TARGET_COL]\n",
        "\n",
        "numeric_cols = get_numeric_cols(df)\n",
        "if not numeric_cols:\n",
        "    raise ValueError(\"No se encontraron columnas numéricas para análisis\")\n",
        "\n",
        "print(f\" Columnas numéricas para análisis: {numeric_cols}\")\n",
        "\n",
        "# 8.2 Matriz de Correlación (con tamaño de letra ajustado)\n",
        "print(\"\\n Matriz de Correlación:\")\n",
        "corr_matrix = df.select(numeric_cols + [TARGET_COL]).to_pandas().corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "heatmap = sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    annot_kws={'size': 8},  # Tamaño de fuente reducido\n",
        "    fmt=\".2f\",\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={\"label\": \"Coeficiente de Correlación\", \"shrink\": 0.75}\n",
        ")\n",
        "\n",
        "# Ajustar tamaño de letra de los ejes\n",
        "heatmap.set_xticklabels(heatmap.get_xticklabels(),\n",
        "                       rotation=45,\n",
        "                       ha='right',\n",
        "                       fontsize=9)\n",
        "\n",
        "heatmap.set_yticklabels(heatmap.get_yticklabels(),\n",
        "                       rotation=0,\n",
        "                       fontsize=9)\n",
        "\n",
        "plt.title('Matriz de Correlación', pad=20, fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8.3 Análisis de Varianza\n",
        "print(\"\\n Análisis de Varianza:\")\n",
        "variance_df = df.select([pl.col(col).var().alias(col) for col in numeric_cols])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(data=variance_df.transpose().to_pandas(), palette='viridis')\n",
        "plt.title('Varianza por Variable', pad=20, fontsize=14)\n",
        "plt.xlabel('Variables')\n",
        "plt.ylabel('Varianza')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Añadir etiquetas de valor\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height():.2f}\",\n",
        "                (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 10),\n",
        "                textcoords='offset points',\n",
        "                fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. Modelado KNN (Versión con manejo de desbalance)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" MODELADO KNN \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 9.1 Preparación de datos\n",
        "X = df.select(numeric_cols).to_numpy()\n",
        "y = df[TARGET_COL].to_numpy()\n",
        "\n",
        "# Discretización de la variable objetivo para análisis de balance\n",
        "# Convertir a Pandas para usar pd.cut\n",
        "y_pd = pd.Series(y)\n",
        "y_binned = pd.cut(y_pd, bins=3, labels=['Low', 'Medium', 'High'])\n",
        "class_distribution = y_binned.value_counts(normalize=True)\n",
        "\n",
        "print(\"\\n Distribución de clases (discretizadas):\")\n",
        "print(class_distribution)\n",
        "\n",
        "# 9.1.1 Verificar desbalance significativo\n",
        "if class_distribution.max() > 0.7:  # Si una clase tiene más del 70%\n",
        "    print(\"\\n Advertencia: Distribución desbalanceada detectada\")\n",
        "    print(\" Aplicando técnicas de balanceo...\")\n",
        "\n",
        "    # Convertir a DataFrame para balanceo\n",
        "    df_pd = df.select(numeric_cols + [TARGET_COL]).to_pandas()\n",
        "    df_pd['target_binned'] = pd.cut(df_pd[TARGET_COL], bins=3, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "    # Separar por clases\n",
        "    majority_class = class_distribution.idxmax()\n",
        "    minority_classes = [c for c in class_distribution.index if c != majority_class]\n",
        "\n",
        "    df_majority = df_pd[df_pd.target_binned == majority_class]\n",
        "    df_minorities = [df_pd[df_pd.target_binned == c] for c in minority_classes]\n",
        "\n",
        "    # Balancear todas las clases\n",
        "    n_samples = min(len(df_majority), *[len(df) for df in df_minorities])\n",
        "\n",
        "    # Downsample majority class\n",
        "    df_majority_downsampled = df_majority.sample(n=n_samples, random_state=42)\n",
        "\n",
        "    # Upsample minority classes\n",
        "    df_minorities_upsampled = [df.sample(n=n_samples, replace=True, random_state=42)\n",
        "                              for df in df_minorities]\n",
        "\n",
        "    # Combinar\n",
        "    df_balanced = pd.concat([df_majority_downsampled] + df_minorities_upsampled)\n",
        "\n",
        "    # Ver nueva distribución\n",
        "    print(\"\\n📊 Nueva distribución después del balanceo:\")\n",
        "    print(df_balanced.target_binned.value_counts(normalize=True))\n",
        "\n",
        "    # Preparar datos balanceados\n",
        "    X = df_balanced[numeric_cols].values\n",
        "    y = df_balanced[TARGET_COL].values\n",
        "else:\n",
        "    print(\"\\n Los datos están balanceados, procediendo sin ajustes\")\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# División train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9.2 Búsqueda del mejor k\n",
        "print(\"\\n Buscando el mejor valor de k...\")\n",
        "k_range = range(1, 21)\n",
        "rmse_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Corrección aplicada aquí\n",
        "    rmse_scores.append(rmse)\n",
        "    print(f\"k={k:2d}: RMSE = {rmse:.4f}\")\n",
        "\n",
        "# Visualización del RMSE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, rmse_scores, marker='o', linestyle='--', color='royalblue')\n",
        "plt.title('RMSE para diferentes valores de k', pad=15)\n",
        "plt.xlabel('Número de vecinos (k)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "best_k = k_range[np.argmin(rmse_scores)]\n",
        "print(f\"\\n Mejor valor de k: {best_k} con RMSE = {min(rmse_scores):.4f}\")\n",
        "\n",
        "# 9.3 Modelo final (Versión Corregida)\n",
        "print(\"\\n Entrenando modelo final con el mejor k...\")\n",
        "final_knn = KNeighborsRegressor(n_neighbors=best_k)\n",
        "final_knn.fit(X_train, y_train)\n",
        "final_preds = final_knn.predict(X_test)\n",
        "\n",
        "# Métricas finales (Corrección aplicada aquí)\n",
        "final_r2 = r2_score(y_test, final_preds)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, final_preds))  # Calculamos RMSE manualmente\n",
        "\n",
        "print(\"\\n Resultados del modelo final:\")\n",
        "print(f\"- R²: {final_r2:.4f}\")\n",
        "print(f\"- RMSE: {final_rmse:.4f}\")\n",
        "\n",
        "# Visualización de predicciones vs reales\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_test, y=final_preds, alpha=0.6, color='royalblue')\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)],\n",
        "         linestyle='--', color='red', linewidth=1)\n",
        "plt.title('Predicciones vs Valores Reales', pad=15)\n",
        "plt.xlabel('Valores Reales')\n",
        "plt.ylabel('Predicciones')\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" ANÁLISIS COMPLETADO \".center(50, \"=\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "# 9.4 Visualización de Errores (Alternativa a Matriz de Confusión para Regresión)\n",
        "print(\"\\n Visualización de Errores de Predicción\")\n",
        "\n",
        "# Crear bins para discretizar las predicciones y valores reales\n",
        "bins = np.linspace(min(y_test.min(), final_preds.min()),\n",
        "                   max(y_test.max(), final_preds.max()), 10)\n",
        "\n",
        "y_test_binned = np.digitize(y_test, bins)\n",
        "preds_binned = np.digitize(final_preds, bins)\n",
        "\n",
        "# Crear matriz de conteo\n",
        "confusion_matrix = np.zeros((len(bins)+1, len(bins)+1))\n",
        "for true, pred in zip(y_test_binned, preds_binned):\n",
        "    confusion_matrix[true-1, pred-1] += 1\n",
        "\n",
        "# Visualización\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='.0f', cmap='Blues',\n",
        "            xticklabels=[f\"{bins[i-1]:.1f}-{bins[i]:.1f}\" for i in range(len(bins))],\n",
        "            yticklabels=[f\"{bins[i-1]:.1f}-{bins[i]:.1f}\" for i in range(len(bins))])\n",
        "plt.title('Distribución de Predicciones vs Valores Reales', pad=20)\n",
        "plt.xlabel('Predicciones (binned)')\n",
        "plt.ylabel('Valores Reales (binned)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Alternativa: Gráfico de dispersión con residuos\n",
        "plt.figure(figsize=(12, 6))\n",
        "residuals = y_test - final_preds\n",
        "sns.scatterplot(x=final_preds, y=residuals, alpha=0.6, color='royalblue')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('Análisis de Residuos', pad=15)\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Residuos (Real - Predicción)')\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" ANÁLISIS COMPLETADO \".center(50, \"=\"))\n",
        "print(\" Hecho por: Jannet Ortiz Aguilar\".center(50, \"=\"))\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# IMPORTACIÓN DE LIBRERÍAS\n",
        "# =============================================\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =============================================\n",
        "# CONFIGURACIÓN INICIAL\n",
        "# =============================================\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette(\"viridis\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# =============================================\n",
        "# CARGA Y PREPROCESAMIENTO DE DATOS\n",
        "# =============================================\n",
        "print(\"=== CARGANDO Y PREPROCESANDO DATOS ===\")\n",
        "\n",
        "# Cargar datos\n",
        "df = pl.read_csv(\"/content/Students_Grading_Dataset.csv\")\n",
        "\n",
        "# Eliminar columnas irrelevantes\n",
        "cols_to_drop = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
        "df = df.drop([col for col in cols_to_drop if col in df.columns])\n",
        "\n",
        "# Codificación de variables categóricas\n",
        "categorical_cols = [\n",
        "    \"Gender\", \"Department\", \"Grade\", \"Extracurricular_Activities\",\n",
        "    \"Internet_Access_at_Home\", \"Parent_Education_Level\", \"Family_Income_Level\"\n",
        "]\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        df = df.with_columns(pl.Series(name=col, values=le.fit_transform(df[col].to_list())))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "class_names = label_encoders[\"Grade\"].classes_\n",
        "\n",
        "# Separar variables predictoras y objetivo\n",
        "X = df.drop(\"Grade\").to_numpy()\n",
        "y = df[\"Grade\"].to_numpy()\n",
        "\n",
        "# División del conjunto de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nDatos después de división:\")\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# Balanceo con SMOTE\n",
        "print(\"\\n=== APLICANDO SMOTE ===\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"\\nDatos después de SMOTE:\")\n",
        "print(f\"X_train_balanced: {X_train_balanced.shape}, y_train_balanced: {y_train_balanced.shape}\")\n",
        "\n",
        "# Normalización\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# =============================================\n",
        "# BÚSQUEDA DE HIPERPARÁMETROS\n",
        "# =============================================\n",
        "print(\"\\n=== BUSCANDO MEJORES HIPERPARÁMETROS ===\")\n",
        "\n",
        "param_grid = {\n",
        "    \"n_neighbors\": list(range(3, 15, 2)),\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train_balanced)\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "print(f\"\\nMejor modelo: {grid_search.best_params_}\")\n",
        "print(f\"Accuracy (CV): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# =============================================\n",
        "# EVALUACIÓN DEL MODELO\n",
        "# =============================================\n",
        "print(\"\\n=== EVALUANDO MODELO ===\")\n",
        "\n",
        "# Generar predicciones\n",
        "y_pred = best_knn.predict(X_test_scaled)\n",
        "\n",
        "# Verificar dimensiones\n",
        "print(f\"\\nDimensiones para evaluación:\")\n",
        "print(f\"X_test_scaled: {X_test_scaled.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(f\"y_pred: {y_pred.shape}\")\n",
        "\n",
        "# Reporte de clasificación\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nReporte de Clasificación:\\n\", classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Matriz de confusión\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar=False)\n",
        "plt.title(\"Matriz de Confusión\", pad=20, fontsize=14)\n",
        "plt.xlabel(\"Predicho\", fontsize=12)\n",
        "plt.ylabel(\"Real\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================\n",
        "# ANÁLISIS DE ERRORES (VERSIÓN CORREGIDA)\n",
        "# =============================================\n",
        "error_df = pl.DataFrame({\n",
        "    \"Real\": label_encoders[\"Grade\"].inverse_transform(y_test),\n",
        "    \"Predicho\": label_encoders[\"Grade\"].inverse_transform(y_pred)\n",
        "}).with_columns(\n",
        "    correcto = pl.col(\"Real\") == pl.col(\"Predicho\")\n",
        ")\n",
        "\n",
        "# Versión corregida usando group_by (Polars) en lugar de groupby (Pandas)\n",
        "error_counts = (error_df.filter(pl.col(\"correcto\") == False)\n",
        "               .group_by(\"Real\")\n",
        "               .agg(pl.count().alias(\"count\"))\n",
        "               .sort(\"count\", descending=True))\n",
        "\n",
        "# Convertir a Pandas para la visualización con Seaborn\n",
        "error_counts_pd = error_counts.to_pandas()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(data=error_counts_pd, x=\"Real\", y=\"count\")\n",
        "plt.title(\"Distribución de Errores por Clase Real\", fontsize=14)\n",
        "plt.xlabel(\"Clase Real\", fontsize=12)\n",
        "plt.ylabel(\"Cantidad de Errores\", fontsize=12)\n",
        "ax.bar_label(ax.containers[0], fontsize=10)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================\n",
        "# EVALUACIÓN ADICIONAL DE MÉTRICAS\n",
        "# =============================================\n",
        "print(\"\\n=== EVALUANDO DIFERENTES MÉTRICAS ===\")\n",
        "\n",
        "k_values = range(1, 21, 2)\n",
        "distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
        "cv_scores = {}\n",
        "\n",
        "for metric in distance_metrics:\n",
        "    print(f\"\\nMétrica: {metric.upper()}\")\n",
        "    scores = []\n",
        "    for k in tqdm(k_values, desc=f\"k values ({metric})\"):\n",
        "        model = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
        "        cv_scores_metric = cross_val_score(model, X_train_scaled, y_train_balanced, cv=5, scoring='accuracy')\n",
        "        scores.append(cv_scores_metric.mean())\n",
        "\n",
        "    cv_scores[metric] = scores\n",
        "    best_k = k_values[np.argmax(scores)]\n",
        "    print(f\"Mejor k: {best_k} | Accuracy: {max(scores):.4f}\")\n",
        "\n",
        "# Visualización comparativa\n",
        "plt.figure(figsize=(12, 6))\n",
        "for metric, scores in cv_scores.items():\n",
        "    plt.plot(k_values, scores, marker='o', linestyle='--', label=metric, linewidth=2)\n",
        "\n",
        "plt.title(\"Comparación de Métricas de Distancia\", fontsize=14)\n",
        "plt.xlabel(\"Valor de k\", fontsize=12)\n",
        "plt.ylabel(\"Accuracy Promedio (CV)\", fontsize=12)\n",
        "plt.legend(title=\"Métrica\", title_fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_values)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== ANÁLISIS COMPLETADO CON ÉXITO ===\")\n",
        "\n",
        "print(\"\")\n",
        "print (\"Jannet Ortiz Aguilar\")"
      ],
      "metadata": {
        "id": "04IWSH8tjg67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}